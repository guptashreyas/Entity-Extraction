{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfcbece4",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) Notebook\n",
    "This notebook provides a detailed exploratory data analysis (EDA) of the dataset. The analysis includes:\n",
    "- **Data Loading and Inspection**: Understanding the structure of the data.\n",
    "- **Missing Value Analysis**: Detecting and handling missing data.\n",
    "- **Statistical Summary and Data Insights**: Gaining insights from summary statistics and visualizations.\n",
    "- **Data Cleaning and Preparation**: Removing or adjusting problematic data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fb548a",
   "metadata": {},
   "source": [
    "##  Import Libraries\n",
    "\n",
    "In this section, we import all the necessary libraries used for data processing, image handling, model building, and evaluation. This includes Pandas and NumPy for data manipulation, OpenCV for image processing, PyTorch for building and training the neural network model, and Transformers for BERT-based NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3287ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9075114",
   "metadata": {},
   "source": [
    "## Load a Sample File and Inspect the Data\n",
    "\n",
    "Here, we load a sample data file in TSV format to inspect its structure and data fields. This helps us understand the type of information available, such as text transcripts and bounding box coordinates, so we can plan further data processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac144b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data with corrected delimiter:\n",
      "   start_index  end_index  x_top_left  y_top_left  x_bottom_right  \\\n",
      "0           33         33         215           4             227   \n",
      "1           35         44         235           3             308   \n",
      "2           46         51         311           3             349   \n",
      "3           53         60         352           3             401   \n",
      "4           62         67         404           3             457   \n",
      "\n",
      "   y_bottom_right  transcript  field  \n",
      "0              21           a  OTHER  \n",
      "1              21  Employee's  OTHER  \n",
      "2              20      social  OTHER  \n",
      "3              20    security  OTHER  \n",
      "4              21      number  OTHER  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 520 entries, 0 to 519\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   start_index     520 non-null    int64 \n",
      " 1   end_index       520 non-null    int64 \n",
      " 2   x_top_left      520 non-null    int64 \n",
      " 3   y_top_left      520 non-null    int64 \n",
      " 4   x_bottom_right  520 non-null    int64 \n",
      " 5   y_bottom_right  520 non-null    int64 \n",
      " 6   transcript      520 non-null    object\n",
      " 7   field           520 non-null    object\n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 32.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the path to the sample TSV file\n",
    "path_to_tsv_folder = 'dataset/train/boxes_transcripts_labels'\n",
    "\n",
    "# Load the file with comma as the delimiter\n",
    "sample_file_path = os.path.join(path_to_tsv_folder, os.listdir(path_to_tsv_folder)[0])\n",
    "sample_data = pd.read_csv(sample_file_path, sep=',', names=[\n",
    "    'start_index', 'end_index', 'x_top_left', 'y_top_left', \n",
    "    'x_bottom_right', 'y_bottom_right', 'transcript', 'field'\n",
    "])\n",
    "\n",
    "# Display the loaded data\n",
    "print(\"Sample Data with corrected delimiter:\")\n",
    "print(sample_data.head())\n",
    "print(sample_data.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91339038",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "In this section, we analyze key characteristics of the data, including the distribution of different entity types (fields), the average length of transcripts, and the size of bounding boxes. This information can be useful in selecting model parameters and understanding the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bb34224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field Distribution:\n",
      " field\n",
      "OTHER                                 491\n",
      "employerAddressStreet_name              5\n",
      "employerName                            3\n",
      "box4SocialSecurityTaxWithheld           3\n",
      "box17StateIncomeTax                     3\n",
      "box1WagesTipsAndOtherCompensations      2\n",
      "box3SocialSecurityWages                 2\n",
      "employeeName                            2\n",
      "box16StateWagesTips                     2\n",
      "ssnOfEmployee                           1\n",
      "einEmployerIdentificationNumber         1\n",
      "box2FederalIncomeTaxWithheld            1\n",
      "employerAddressZip                      1\n",
      "employerAddressCity                     1\n",
      "employerAddressState                    1\n",
      "taxYear                                 1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check distribution of 'field' values\n",
    "field_distribution = sample_data['field'].value_counts()\n",
    "print(\"Field Distribution:\\n\", field_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4077f445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average token length: 4.961538461538462\n",
      "Bounding Box Area Stats:\n",
      " count     520.000000\n",
      "mean      908.030769\n",
      "std       874.764283\n",
      "min        84.000000\n",
      "25%       374.750000\n",
      "50%       674.000000\n",
      "75%      1073.250000\n",
      "max      7950.000000\n",
      "Name: bbox_area, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Token length analysis\n",
    "sample_data['transcript_length'] = sample_data['transcript'].apply(len)\n",
    "print(\"Average token length:\", sample_data['transcript_length'].mean())\n",
    "\n",
    "# Bounding box area calculation\n",
    "sample_data['bbox_area'] = (sample_data['x_bottom_right'] - sample_data['x_top_left']) * \\\n",
    "                           (sample_data['y_bottom_right'] - sample_data['y_top_left'])\n",
    "print(\"Bounding Box Area Stats:\\n\", sample_data['bbox_area'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe5694c",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation\n",
    "\n",
    "This section focuses on cleaning and preparing the data by standardizing text (e.g., making it lowercase), grouping transcripts for multi-token entities, and making the data ready for tokenization. This step ensures consistency and accuracy in the data fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85bf8b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean transcript tokens\n",
    "sample_data['transcript'] = sample_data['transcript'].str.lower().str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be403c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped Data:\n",
      "                                 field  \\\n",
      "0                               OTHER   \n",
      "1                 box16StateWagesTips   \n",
      "2                 box17StateIncomeTax   \n",
      "3  box1WagesTipsAndOtherCompensations   \n",
      "4        box2FederalIncomeTaxWithheld   \n",
      "5             box3SocialSecurityWages   \n",
      "6       box4SocialSecurityTaxWithheld   \n",
      "7     einEmployerIdentificationNumber   \n",
      "8                        employeeName   \n",
      "9                 employerAddressCity   \n",
      "\n",
      "                                          transcript  \n",
      "0  a employee's social security number safe, accu...  \n",
      "1                                          20287. 85  \n",
      "2                                          1690 . 44  \n",
      "3                                          41669. 07  \n",
      "4                                           11182.93  \n",
      "5                                          53826. 13  \n",
      "6                                           4117 . 7  \n",
      "7                                         37-3493491  \n",
      "8                                   stephanie dawson  \n",
      "9                                     rodriguezmouth  \n"
     ]
    }
   ],
   "source": [
    "# Group by field to combine multi-token entities\n",
    "grouped_data = sample_data.groupby('field')['transcript'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "print(\"Grouped Data:\\n\", grouped_data.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c1a71ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding box area feature (already calculated)\n",
    "sample_data['bbox_area'] = (sample_data['x_bottom_right'] - sample_data['x_top_left']) * \\\n",
    "                           (sample_data['y_bottom_right'] - sample_data['y_top_left'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec9b9531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       204\n",
      "1      1314\n",
      "2       646\n",
      "3       833\n",
      "4       954\n",
      "       ... \n",
      "515     442\n",
      "516     680\n",
      "517     289\n",
      "518     774\n",
      "519     540\n",
      "Name: bbox_area, Length: 520, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(sample_data['bbox_area'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab256bee",
   "metadata": {},
   "source": [
    "## Model build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001d40e3",
   "metadata": {},
   "source": [
    "## Label Mapping\n",
    "\n",
    "We define a mapping of field names (entities) to integer labels, which is necessary for model training. The model will learn to associate these integer labels with different fields in the dataset, enabling it to classify new data points into the correct categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "254efb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the label mapping for the entity labels in your dataset\n",
    "label_map = {\n",
    "    'employerName': 0,\n",
    "    'employerAddressStreet_name': 1,\n",
    "    'employerAddressCity': 2,\n",
    "    'employerAddressState': 3,\n",
    "    'employerAddressZip': 4,\n",
    "    'einEmployerIdentificationNumber': 5,\n",
    "    'employeeName': 6,\n",
    "    'ssnOfEmployee': 7,\n",
    "    'box1WagesTipsAndOtherCompensations': 8,\n",
    "    'box2FederalIncomeTaxWithheld': 9,\n",
    "    'box3SocialSecurityWages': 10,\n",
    "    'box4SocialSecurityTaxWithheld': 11,\n",
    "    'box16StateWagesTips': 12,\n",
    "    'box17StateIncomeTax': 13,\n",
    "    'taxYear': 14,\n",
    "    'OTHER': 15  # Label for non-entity tokens\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8143f74",
   "metadata": {},
   "source": [
    "## Tokenization and Coordinate Preparation\n",
    "\n",
    "This section involves preparing the data for the model by tokenizing transcripts using BERT and creating tensors for bounding box coordinates and labels. The tokenized text and coordinates will be used as inputs to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9498e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the transcripts and prepare coordinate features\n",
    "def prepare_data(data, label_map):\n",
    "    tokens = tokenizer(data['transcript'].tolist(), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Prepare bounding box tensor\n",
    "    bbox = torch.tensor(data[['x_top_left', 'y_top_left', 'x_bottom_right', 'y_bottom_right']].values)\n",
    "    \n",
    "    # Map field names to integer labels\n",
    "    labels = torch.tensor(data['field'].map(label_map).values)\n",
    "    \n",
    "    return tokens, bbox, labels\n",
    "\n",
    "# # Example usage\n",
    "# tokens, bbox, labels = prepare_data(sample_data, label_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cbda85",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "We define a custom BERT model that combines BERT embeddings with bounding box coordinates. This allows the model to use both the text and spatial information to make predictions, which is helpful for tasks involving both text and visual cues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ab01376",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BERTWithCoords(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(BERTWithCoords, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size + 4, num_labels)  # +4 for bbox coordinates\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, bbox):\n",
    "        # Get BERT embeddings\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Concatenate BERT embeddings with bounding box features\n",
    "        combined_features = torch.cat((bert_output.pooler_output, bbox), dim=1)\n",
    "        \n",
    "        # Classification layer\n",
    "        logits = self.fc(combined_features)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922eed25",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Here, we implement the training loop for the model. The loop involves a forward pass to generate predictions, calculating the loss, backpropagation, and updating model parameters. This process is repeated over several epochs to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69c4cca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 65/65 [09:23<00:00,  8.67s/it, loss=118] \n",
      "Epoch 1: 100%|██████████| 65/65 [09:19<00:00,  8.61s/it, loss=80.7]\n",
      "Epoch 2: 100%|██████████| 65/65 [35:19<00:00, 32.61s/it, loss=67.8]\n"
     ]
    }
   ],
   "source": [
    "# Custom Dataset class\n",
    "class EntityDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, label_map):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_map = label_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Tokenize the transcript\n",
    "        tokens = self.tokenizer(row['transcript'], return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "        \n",
    "        # Bounding box tensor and label\n",
    "        bbox = torch.tensor([row['x_top_left'], row['y_top_left'], row['x_bottom_right'], row['y_bottom_right']])\n",
    "        label = torch.tensor(self.label_map[row['field']])\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": tokens['input_ids'].squeeze(),  # Remove extra dimensions\n",
    "            \"attention_mask\": tokens['attention_mask'].squeeze(),\n",
    "            \"bbox\": bbox,\n",
    "            \"labels\": label\n",
    "        }\n",
    "\n",
    "# Initialize DataLoader\n",
    "dataset = EntityDataset(sample_data, tokenizer, label_map)\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Initialize model, optimizer, and training loop\n",
    "model = BERTWithCoords(num_labels=len(label_map))\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, data_loader, optimizer, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(data_loader, leave=True)\n",
    "        for batch in loop:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get inputs from the batch\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            bbox = batch['bbox']\n",
    "            labels = batch['labels']\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, bbox=bbox)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Log the loss\n",
    "            loop.set_description(f'Epoch {epoch}')\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# Start training\n",
    "train_model(model, data_loader, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "975d742d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to bert_with_coords_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model with a .pth extension\n",
    "model_save_path = \"bert_with_coords_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
